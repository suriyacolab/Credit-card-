{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "# --------------------------------------\n",
        "# 1. Import Libraries\n",
        "# --------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import gradio as gr\n",
        "import os"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "znQ2fm9QcpgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# 2. Load and Inspect Dataset\n",
        "# --------------------------------------\n",
        "def load_data(path: str, target_column: str = \"Class\") -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    if target_column not in df.columns:\n",
        "        # Raise a more informative KeyError if the target column is not found\n",
        "        raise KeyError(f\"Target column '{target_column}' not found. Available columns: {df.columns.tolist()}\")\n",
        "    print(\"Original Class Distribution:\")\n",
        "    print(df[target_column].value_counts())\n",
        "    return df"
      ],
      "metadata": {
        "id": "g7f7-qtOiLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Preprocessing\n",
        "# --------------------------------------\n",
        "def preprocess_data(df: pd.DataFrame, target_column: str = \"Is_Fraud\"): # Updated default target_column\n",
        "    if target_column not in df.columns:\n",
        "        raise KeyError(f\"Target column '{target_column}' not found in DataFrame.\")\n",
        "\n",
        "    # Drop non-numeric columns that are not features or the target\n",
        "    # Assuming 'Transaction_ID' and 'Customer_ID' are columns to drop\n",
        "    cols_to_drop = ['Transaction_ID', 'Customer_ID']\n",
        "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
        "\n",
        "    X = df.drop(target_column, axis=1)\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Identify and drop non-numeric columns from X before scaling and training\n",
        "    non_numeric_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    if non_numeric_cols:\n",
        "        print(f\"Dropping non-numeric columns: {non_numeric_cols}\")\n",
        "        X = X.drop(columns=non_numeric_cols)\n",
        "\n",
        "    cols_to_scale = [col for col in ['Time', 'Amount'] if col in X.columns]\n",
        "    scaler = StandardScaler() if cols_to_scale else None\n",
        "    if scaler:\n",
        "        # Ensure columns to scale are numeric before applying scaler\n",
        "        numeric_cols_to_scale = X[cols_to_scale].select_dtypes(include=np.number).columns.tolist()\n",
        "        if numeric_cols_to_scale:\n",
        "             X[numeric_cols_to_scale] = scaler.fit_transform(X[numeric_cols_to_scale])\n",
        "        else:\n",
        "             print(\"Warning: 'Time' or 'Amount' columns are not numeric and cannot be scaled.\")\n",
        "\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    try:\n",
        "        # Ensure X and y are suitable for SMOTE (all numeric for X, numerical or categorical for y)\n",
        "        if X.select_dtypes(exclude=np.number).empty:\n",
        "            X_res, y_res = smote.fit_resample(X, y)\n",
        "            print(\"Balanced Class Distribution:\")\n",
        "            print(pd.Series(y_res).value_counts())\n",
        "        else:\n",
        "            print(\"SMOTE cannot be applied due to non-numeric features in X after dropping. Proceeding without resampling.\")\n",
        "            X_res, y_res = X, y\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"SMOTE error: {e}. Proceeding without resampling.\")\n",
        "        X_res, y_res = X, y\n",
        "    return X_res, y_res, scaler\n"
      ],
      "metadata": {
        "id": "KSunbwaeiTGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train Model\n",
        "# --------------------------------------\n",
        "def train_model(X, y):\n",
        "    if X.empty or y.empty or len(X) != len(y):\n",
        "        raise ValueError(\"Training data is empty or mismatched.\")\n",
        "\n",
        "    # Ensure X contains only numeric data before splitting and training\n",
        "    if not X.select_dtypes(exclude=np.number).empty:\n",
        "        non_numeric_cols_in_X = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "        raise ValueError(f\"Input data for training contains non-numeric columns after preprocessing: {non_numeric_cols_in_X}. Please ensure all feature columns are numeric.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model, X_test, y_test\n",
        "\n",
        "# Rest of the code remains the same...\n"
      ],
      "metadata": {
        "id": "fTbFawTtia0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluation\n",
        "# --------------------------------------\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    if X_test.empty or y_test.empty:\n",
        "        print(\"Warning: Empty test data.\")\n",
        "        return\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    if hasattr(model, 'predict_proba') and len(np.unique(y_test)) > 1:\n",
        "        # Check if the target variable has more than one class before calculating ROC-AUC\n",
        "        if len(np.unique(y_test)) > 1:\n",
        "             # Ensure X_test is numeric before predicting probabilities\n",
        "             if X_test.select_dtypes(exclude=np.number).empty:\n",
        "                 print(\"ROC-AUC Score:\", roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
        "             else:\n",
        "                 print(\"ROC-AUC Score cannot be calculated: X_test contains non-numeric columns.\")\n",
        "        else:\n",
        "             print(\"ROC-AUC Score cannot be calculated as there is only one class in y_test.\")\n",
        "    else:\n",
        "        print(\"ROC-AUC Score cannot be calculated (model does not have predict_proba or only one class).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QdCvQEQDih-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Gradio Interface\n",
        "# --------------------------------------\n",
        "def create_interface(model, scaler):\n",
        "    if model is None:\n",
        "        print(\"Cannot create Gradio interface: Model is not trained.\")\n",
        "        return\n",
        "\n",
        "    def predict_fraud(time, amount, *features):\n",
        "        try:\n",
        "            scaled_vals = scaler.transform([[time, amount]])[0] if scaler else [time, amount]\n",
        "        except Exception as e:\n",
        "            print(f\"Scaling error: {e}\")\n",
        "            scaled_vals = [time, amount]\n",
        "\n",
        "        # Ensure input data has the same number of features as the training data\n",
        "        if hasattr(model, 'feature_names_in_'):\n",
        "            all_model_features = model.feature_names_in_.tolist()\n",
        "            # Reconstruct the input list based on model's feature names\n",
        "            # Handle Time and Amount first, then add the V features in the order expected by the model\n",
        "            input_data_dict = {'Time': scaled_vals[0], 'Amount': scaled_vals[1]}\n",
        "            # Assuming 'features' correspond to V1, V2, ... V28 in order\n",
        "            v_feature_names_in_model = [f for f in all_model_features if f.startswith('V')]\n",
        "            for i, v_feat_name in enumerate(v_feature_names_in_model):\n",
        "                if i < len(features):\n",
        "                    input_data_dict[v_feat_name] = features[i]\n",
        "                else:\n",
        "                    # Handle case where fewer V features are provided than expected\n",
        "                    # This might indicate an issue with the interface inputs or the model's expectations\n",
        "                    print(f\"Warning: Missing value for expected feature '{v_feat_name}'. Using 0 as placeholder.\")\n",
        "                    input_data_dict[v_feat_name] = 0 # Placeholder, consider better handling\n",
        "\n",
        "            # Create the input array ensuring the order matches the model's feature_names_in_\n",
        "            input_data_list = [input_data_dict.get(feat, 0) for feat in all_model_features] # Use 0 as placeholder for missing features\n",
        "            input_data = np.array([input_data_list])\n",
        "\n",
        "            if input_data.shape[1] != len(all_model_features):\n",
        "                 raise ValueError(f\"Input shape mismatch. Expected {len(all_model_features)} features, got {input_data.shape[1]}. Ensure all required features are provided and in the correct format.\")\n",
        "\n",
        "        else:\n",
        "            # Fallback if feature_names_in_ is not available (e.g., older sklearn versions)\n",
        "            # Assuming the order is Time, Amount, then V1 to V28\n",
        "            input_data = np.array([[scaled_vals[0], scaled_vals[1]] + list(features)])\n",
        "            # Add a check here if possible based on the expected number of features\n",
        "\n",
        "        # Ensure input_data is numeric before prediction\n",
        "        try:\n",
        "             input_data = input_data.astype(float)\n",
        "        except ValueError as e:\n",
        "             return \"Error\", f\"Input data contains non-numeric values: {e}\"\n",
        "\n",
        "\n",
        "        prediction = model.predict(input_data)[0]\n",
        "        # Check if the model has predict_proba and the target has two classes\n",
        "        if hasattr(model, 'predict_proba') and model.classes_.shape[0] > 1:\n",
        "             prob = model.predict_proba(input_data)[0][1]\n",
        "             confidence_text = f\"Confidence: {prob:.2%}\"\n",
        "        else:\n",
        "             prob = None\n",
        "             confidence_text = \"Confidence information not available (model does not have predict_proba or target is not binary).\"\n",
        "\n",
        "\n",
        "        return \"Fraudulent\" if prediction == 1 else \"Legitimate\", confidence_text\n",
        "\n",
        "    if hasattr(model, 'feature_names_in_'):\n",
        "        all_features_in_model = model.feature_names_in_.tolist()\n",
        "        # Assuming Time and Amount are the first two features if feature_names_in_ is available\n",
        "        # Also exclude 'Transaction_ID' and 'Customer_ID' which were dropped during preprocessing\n",
        "        # and are not expected as inputs to the Gradio interface\n",
        "        v_features_in_model = [f for f in all_features_in_model if f.startswith('V')]\n",
        "        # Ensure the order for Gradio inputs matches the model's expectations (Time, Amount, then V features in order)\n",
        "        gr_input_feature_names = [f for f in ['Time', 'Amount'] if f in all_features_in_model] + v_features_in_model\n",
        "    else:\n",
        "        # Fallback for older sklearn versions, assuming Time, Amount, then V1 to V28\n",
        "        gr_input_feature_names = ['Time', 'Amount'] + [f\"V{i}\" for i in range(1, 29)] # Assuming V1 to V28\n",
        "        # This fallback might be inaccurate if the actual data has other non-V columns besides Time and Amount\n",
        "\n",
        "    gr_inputs = [gr.Number(label=f) for f in gr_input_feature_names]\n",
        "    gr_outputs = [gr.Text(label=\"Prediction\"), gr.Text(label=\"Confidence\")]\n",
        "\n",
        "\n",
        "    gr.Interface(\n",
        "        fn=predict_fraud,\n",
        "        inputs=gr_inputs,\n",
        "        outputs=gr_outputs,\n",
        "        title=\"ðŸ’³ AI-Powered Credit Card Fraud Detection\",\n",
        "        description=\"Input transaction details to check if it's fraudulent or legitimate.\"\n",
        "    ).launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7U-_P4n3jEVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Run Full Pipeline\n",
        "# --------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file_path = \"credit_card_fraud_dataset.csv\"\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        print(f\"Error: Dataset file not found at '{csv_file_path}'.\")\n",
        "    else:\n",
        "        # Change target_column from \"Class\" to \"Is_Fraud\"\n",
        "        df = load_data(csv_file_path, target_column=\"Is_Fraud\")\n",
        "        # Change target_column from \"Class\" to \"Is_Fraud\"\n",
        "        X_res, y_res, scaler = preprocess_data(df, target_column=\"Is_Fraud\")\n",
        "        model, X_test, y_test = train_model(X_res, y_res)\n",
        "        evaluate_model(model, X_test, y_test)\n",
        "        create_interface(model, scaler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65gNiBBRjMG2",
        "outputId": "17f74b2e-fcdc-4d5b-cfaf-568e1fe64907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Class Distribution:\n",
            "Is_Fraud\n",
            "0    3793\n",
            "1     207\n",
            "Name: count, dtype: int64\n",
            "Dropping non-numeric columns: ['Timestamp', 'Merchant_ID', 'Merchant_Category', 'Transaction_Type', 'Location_City', 'Location_Country', 'Device_ID', 'Channel', 'Is_3DS_Authenticated', 'Previous_Fraud_Flag']\n",
            "Balanced Class Distribution:\n",
            "Is_Fraud\n",
            "0    3793\n",
            "1    3793\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Confusion Matrix:\n",
            "[[445 330]\n",
            " [280 463]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.57      0.59       775\n",
            "           1       0.58      0.62      0.60       743\n",
            "\n",
            "    accuracy                           0.60      1518\n",
            "   macro avg       0.60      0.60      0.60      1518\n",
            "weighted avg       0.60      0.60      0.60      1518\n",
            "\n",
            "ROC-AUC Score: 0.690024747101984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1027: UserWarning: Expected at least 2 arguments for function <function create_interface.<locals>.predict_fraud at 0x7922119f09a0>, received 1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://69354c9b2702f64e89.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://69354c9b2702f64e89.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}